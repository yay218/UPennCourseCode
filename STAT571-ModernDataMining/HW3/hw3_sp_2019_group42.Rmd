---
title: "Modern Data Mining - HW 3"
author:
- Kexin Zhu
- Yang Yi
- Yifan Jiang
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=4, fig.width=6, warning = F)

# constants for homework assignments
hw_num <- 3
hw_due_date <- "17, March, 2019"
```



## Overview / Instructions

This is homework #`r paste(hw_num)` of STAT 471/571/701. It will be **due on `r paste(hw_due_date)` by 11:59 PM** on Canvas. You can directly edit this file to add your answers. Submit the Rmd file, a PDF or word or HTML version with **only 1 submission** per HW team. No zip files please.

**Note:** To minimize your work and errors, we provide this Rmd file to guide you in the process of building your final report. To that end, we've included code to load the necessary data files. Make sure that the following files are in the same folder as this R Markdown file:

* `FRAMINGHAM.dat`
* `Bills.subset.csv`
* `Bills.subset.test.csv`

The data should load properly if you are working in Rstudio, *without needing to change your working directory*.



## R Markdown / Knitr tips

You should think of this R Markdown file as generating a polished report, one that you would be happy to show other people (or your boss). There shouldn't be any extraneous output; all graphs and code run should clearly have a reason to be run. That means that any output in the final file should have explanations.

A few tips:

* Keep each chunk to only output one thing! In R, if you're not doing an assignment (with the `<-` operator), it's probably going to print something.
* If you don't want to print the R code you wrote (but want to run it, and want to show the results), use a chunk declaration like this: `{r, echo=F}`
* If you don't want to show the results of the R code or the original code, use a chunk declaration like: `{r, include=F}`
* If you don't want to show the results, but show the original code, use a chunk declaration like: `{r, results='hide'}`.
* If you don't want to run the R code at all use `{r, eval = F}`.
* We show a few examples of these options in the below example code. 
* For more details about these R Markdown options, see the [documentation](http://yihui.name/knitr/options/).
* Delete the instructions and this R Markdown section, since they're not part of your overall report.

## Problem 0

Review the code and concepts covered during lecture, in particular, logistic regression and classification. 

## Problem 1
We will continue to use the Framingham Data (`Framingham.dat`) so that you are already familiar with the data and the variables. All the results are obtained through training data.

To keep our answers consistent, use a subset of the data, and exclude anyone with a missing entry. For your convenience, we've loaded it here together with a brief summary about the data.

```{r data preparation, include=F}
# Notice that we hide the code and the results here
# Using `include=F` in the chunk declaration. 
hd_data <- read.csv("Framingham.dat")
str(hd_data) 

### Renames, setting the variables with correct natures...
names(hd_data)[1] <- "HD"
hd_data$HD <- as.factor(hd_data$HD)
hd_data$SEX <- as.factor(hd_data$SEX)
str(hd_data)
#tail(hd_data, 1)    # The last row is for prediction
hd_data.new <- hd_data[1407,] # The female whose HD will be predicted.
hd_data <- hd_data[-1407,]  # take out the last row 
hd_data.f <- na.omit(hd_data)
```

We note that this dataset contains 311 people diagnosed with heart disease and 1095 without heart disease.
```{r table heart disease, echo = F, comment = " "}
# we use echo = F to avoid showing this R code
table(hd_data$HD) # HD: 311 of "0" and 1095 "1" 
```

After a quick cleaning up here is a summary about the data:
```{r data summary, comment="     "}
# using the comment="     ", we get rid of the ## in the output.
summary(hd_data.f)
```
### Part 1A
Conceptual questions to understand the building blocks of logistic regression. All the codes in this part should be hidden. We will use a small subset to run a logistic regression of `HD` vs. `SBP`. 

i. Take a random subsample of size 5 from `hd_data_f` which only includes `HD` and `SBP`. Also set   `set.seed(50)`. List the five observations neatly below. No code should be shown here.

```{r include = FALSE}
N <- length(hd_data.f$HD)

set.seed(50)
index <- sample(N,5)
sample <- hd_data.f[index,]
sample <- sample[,c("HD","SBP")] #extract only HD and SBP
```

```{r echo = FALSE}
sample # to show results here
```

ii. Write down the likelihood function using the five observations above.

\[\begin{split}
\mathcal{Lik}(\beta_0, \beta_1 \vert {\text Data}) &= {\text {Prob(the outcome of the data)}}\\
&=Prob((Y=0|SBP=142), (Y=0|SBP=126), (Y=0|SBP=136), (Y=0|SBP=178), (Y=0|SBP=126))\\
&= \frac{1}{1+e^{\beta_0 + 142 \beta_1}}\cdot\frac{1}{1+e^{\beta_0 + 126\beta_1}}\cdot\frac{1}{1+e^{\beta_0 + 136\beta_1}}\cdot\frac{1}{1+e^{\beta_0 + 178\beta_1}}\cdot\frac{1}{1+e^{\beta_0 + 126\beta_1}}
	\end{split}\]

iii. Find the MLE based on this subset using glm(). Report the estimated logit function of `SBP` and the probability of `HD`=1. Briefly explain how the MLE are obtained based on ii. above.

```{r}
fit <- glm(HD~SBP, sample, family=binomial(logit)) 
summary(fit)
```

Thus, we have:

- logit = -24.57 + 0.00 SBP

- $P(HD = 1 \vert SBP) = \frac{e^{-24.57 + 0.00 \times  SBP}}{1+e^{-24.57 + 0.00 \times SBP}}$

MLE(Max Likelihood Estimator) are obtained by the likelihood function, which requires multipling the max possibility of `HD = 0` or `HD = 1` based on specific conditions that lead to corresponding results. The possibility is estimated by calculating the odds of success in or failure in terms of heart attack.


### Part 1B 


Goal: Identify important risk factors for `Heart.Disease.` through logistic regression. 
Start a fit with just one factor, `SBP`, and call it `fit1`. Let us add one variable to this at a time from among the rest of the variables. 
```{r, results='hide'}
fit1 <- glm(HD~SBP, hd_data.f, family=binomial)
summary(fit1)
fit1.1 <- glm(HD~SBP + AGE, hd_data.f, family=binomial)
summary(fit1.1)
fit1.2 <- glm(HD~SBP + SEX, hd_data.f, family=binomial)
summary(fit1.2)
fit1.3 <- glm(HD~SBP + DBP, hd_data.f, family=binomial)
summary(fit1.3)
fit1.4 <- glm(HD~SBP + CHOL, hd_data.f, family=binomial)
summary(fit1.4)
fit1.5 <- glm(HD~SBP + DBP, hd_data.f, family=binomial)
summary(fit1.5)
fit1.6 <- glm(HD~SBP + FRW, hd_data.f, family=binomial)
summary(fit1.6)
fit1.7 <- glm(HD~SBP + CIG, hd_data.f, family=binomial)
summary(fit1.7)
```



i. Which single variable would be the most important to add? Add it to your model, and call the new fit `fit2`. 

**Answer**: Since we observed that fit1.2 output the smallest `AIC`, we conclude that `SEX` would be the most important to add in the model with two predictors. 

```{r include = F}
# This chunk demonstrates the conclusion above
library(bestglm)
Xy <- model.matrix(HD ~.+0, hd_data.f) 
Xy <- data.frame(Xy, hd_data.f$HD) 
fit.all <- bestglm(Xy, family = binomial, method = "forward", IC="AIC", nvmax = 10)
summary(fit.all)
names(fit.all)
fit.all$Subsets
# We can see two variables (SEX and SBP)
```

```{r}
fit2 <- glm(HD~SBP + SEX, hd_data.f, family=binomial)
summary(fit2)
```


We will pick up the variable either with highest $|z|$ value, or smallest $p$ value. From all the two variable models we see that `SEX` will be the most important addition on top of the SBP. And here is the summary report.

```{r the most important addition, results='asis', comment="   "}
## How to control the summary(fit2) output to cut some junk?
## We could use packages: xtable or broom. 
library(xtable)
options(xtable.comment = FALSE)
fit2 <- glm(HD~SBP + SEX, hd_data.f, family=binomial)
xtable(fit2)
```

ii. Is the residual deviance of `fit2` always smaller than that of `fit1`? Why or why not?

```{r}
fit2$deviance
fit1$deviance
fit2$deviance - fit1$deviance < 0
```

**Answer**: Yes. Residual deviance is a goodness-of-fit statistic in a logit model. The deviance would be larger with fewer variables since we exclude the influence of other variables or meaningful interactions between, which leads to model underfitting. In `fit1`, the variable `SEX` has been restricted, meaning that $\beta$ for `SEX`= 0, and this caused underfitting where the deviance would be larger than that without the restriction.
Deviance = AIC - 2# of parameters, fit2 has smaller AIC than fit1 and fit2 has more parameter.


iii. Perform both the Wald test and the Likelihood ratio tests (Chi-Squared) to see if the added variable is significant at the .01 level.  What are the p-values from each test? Are they the same? 

**Answer**: Wald test (z-test) is shown in the summary chunk:

```{r}
summary(fit1)
confint(fit1, level = .99)

summary(fit2)
confint(fit2, level = .99)


```

The likelihood ratio test is shown as following:
\[\begin{split}
\text {Testing stat} = \chi^2 
& = -2\times \log \frac{\max _{H_1} \mathcal{Lik}(\beta_0, \beta_1 \vert D)}{\max_{H_0} \mathcal{Lik}(\beta_0, \beta_1 \vert D)}\\
&=-2\log(\mathcal{Lik}_{H_0}) - (-2\log(\mathcal{Lik}_{H_1}))\\
&= Null Deviance - Residual Deviance\\
&\sim \chi^2_{df=1}
\end{split}\]

The numbers are also output by the summary function: 

- Null Deviance = 1469.3

- Residual Deviance = 1373.8

- $\chi^2 = 1469.3-1373.8= 95.5$

```{r}
anova(fit1, test="Chisq")

chi.sq <- 1469.3-1373.8
pchisq(chi.sq, 1, lower.tail = FALSE)
anova(fit2, test="Chisq", alpha = .99) 
```

**Answer**:The p-value for both tests are 0.00. They are similar but not exactly the same. However, both tests demonstrate that the added variable `SEX` is significant at .01 level.


### Part 1C -  Model building

Start with all variables. Our goal is to fit a well-fitting model, that is still small and easy to interpret (parsimonious).

i. Use backward selection method. Only keep variables whose coefficients are significantly different from 0 at .05 level. Kick out the variable with the largest p-value first, and then re-fit the model to see if there are other variables you want to kick out.

```{r, results='hide'}
fit.full <- glm(HD~., hd_data.f, family=binomial) 
summary(fit.full)
fit.full.1 <- update(fit.full, .~. -DBP)
summary(fit.full.1)
fit.full.2 <- update(fit.full.1, .~. -FRW)
summary(fit.full.2)
fit.full.3 <- update(fit.full.2, .~. -CIG)
summary(fit.full.3)

fit.full.3.predict <- predict(fit.full.3, hd_data.f, type="response")

fit.bac <- glm(HD~AGE+SEX+SBP+CHOL, hd_data.f, family = binomial)
fit.bac
```

Our model is:

$Logit = -8.41+0.056*Age+0.99*SEX(Male)+0.017*SBP+0.004*CHOL$


ii. Use AIC as the criterion for model selection. Find a model with small AIC through exhaustive search. Does exhaustive search  guarantee that the p-values for all the remaining variables are less than .05? Is our final model here the same as the model from backwards elimination? 

```{r}
library(bestglm)


# Get the design matrix without 1's and HD.
fit.full <- glm(HD~., hd_data.f, family=binomial)
Xy <- model.matrix(HD ~.+0, hd_data.f) 

#Attach y as the last column.
Xy <- data.frame(Xy, hd_data.f$HD)

fit.all <- bestglm(Xy, family = binomial, method = "exhaustive", IC="AIC", nvmax = 10) 
summary(fit.all$BestModel)
fit.final = fit.all$BestModel

```

**Answer**: No. We observed that in our best model, `FRW` is not significant at .05 level since the p-value is larger than .05. Obviously, our final model is not the same as the model from backwards elimination.


iii. Use the model chosen from part ii. as the final model. Write a brief summary to describe important factors relating to Heart Diseases (i.e. the relationships between those variables in the model and heart disease). Give a definition of “important factors”. 

**Answer**: Our model is:

$Logit = -9.23+0.06*AGE+0.91*SEX(Male)+0.016*SBP+0.004*CHOL+0.006*FRW+0.012*CIG$

From the model we can say that, collectively, `AGE`, `SBP`, `CHOL`, `CIG` are all positively related to the chance of a `HD`, although the correlations are weak (the slopes are mild). Specifically, the log of having a heart disease would incrase by 0.016 if SBP increased by 1. The other variables can be interperated in the same way. Also, `Males` would have a higher chance of having heart disease than `Females` while all the other factors are controlled in our model. 

**Notice**: Here, we exclude `FRW` as an important predictor in our model because the p-value of z score is not significant at a .05 level. 



### Part 1D - Prediction
Liz is a patient with the following readings: `AGE=50, GENDER=FEMALE, SBP=110, DBP=80, CHOL=180, FRW=105, CIG=0`. What is the probability that she will have heart disease, according to our final model?

```{r}
no <- 1/(1+exp(-9.23+0.06*50+0.016*110+0.004*180+0.006*105+0.012*0))
yes <- 1-no
yes
```

**Answer**: The probability that Liz will have heart diease is 4.2% according to our final model.



### Part 2 - Classification analysis

a. Display the ROC curve using `fit1`. Explain what ROC reports and how to use the graph. Specify the classifier such that the False Positive rate is less than .1 and the True Positive rate is as high as possible.
```{r}
library(pROC)

fit1.roc<- roc(hd_data.f$HD, fit1$fitted, col="blue")

plot(1-fit1.roc$specificities, fit1.roc$sensitivities, col="red", pch=16,
     xlab="False Positive", 
     ylab="Sensitivity")

```
**Answer**:  ROC curvers measure the performance of such a classifier. ROC curve is the True positive$P(\hat{Y}= 1|Y=1)$ against False Positive$P(\hat{Y}= 1| Y=0)$, the higher true positive will lead to higher False Positive, if true positive is 0, then false positive is also 0. In contrast, if true positive is 1, then false positive is also 1. We can also use AUC (Area under the curve) to measure. It is also used to measure the performance of the classifier as a whole: the larger the better.

b. Overlay two ROC curves: one from `fit1`, the other from `fit2`. Does one curve always contain the other curve? Is the AUC of one curve always larger than the AUC of the other one? Why or why not?
```{r}
fit2.roc<- roc(hd_data.f$HD, fit2$fitted, col="blue")
plot(1-fit1.roc$specificities, fit1.roc$sensitivities, col="red", pch=16, cex=.7, 
     xlab="False Positive", 
     ylab="Sensitivity")
points(1-fit2.roc$specificities, fit2.roc$sensitivities, col="blue", pch=16, cex=.6)
title("Blue line is for fit2, and red for fit1")
```
**Answer**: Yes,fit2 is always contain fit1 and AUC of fit2 is always larger than AUC of fit1. Because ROC increases with more variable, since ROC measures the training data, so it may occur overfitting.

c. Estimate the Positive Prediction Values and Negative Prediction Values for `fit1` and `fit2` using .5 as a threshold. Which model is more desirable if we prioritize the Positive Prediction values?
```{r}
#Positive Prediction for fit 1
fit1.pred.5 <- ifelse(fit1$fitted.values > 0.5, "1", "0")
cm1.5 <- table(fit1.pred.5, hd_data.f$HD)
cm1.5
positive1.pred <- cm1.5[2, 2] / (cm1.5[2, 1] + cm1.5[2, 2])
positive1.pred
#Negative Prediction for fit2
negative1.pred <- cm1.5[1, 1] / (cm1.5[1, 1] + cm1.5[1, 2])
negative1.pred


#Positive Prediction for fit 2
fit2.pred.5 <- ifelse(fit2$fitted.values > 0.5, "1", "0")
cm2.5 <- table(fit2.pred.5, hd_data.f$HD)
cm2.5
positive2.pred <- cm2.5[2, 2] / (cm2.5[2, 1] + cm2.5[2, 2])
positive2.pred
#Negative Prediction
negative2.pred <- cm2.5[1, 1] / (cm2.5[1, 1] + cm2.5[1, 2])
negative2.pred

```
**Answer**: fit2 is more desirable if we prioritize the Positive Prediction values.
d. (Optional/extra credit) For `fit1`: overlay two curves,  but put the threshold over the probability function as the x-axis and positive prediction values and the negative prediction values as the y-axis.  Overlay the same plot for `fit2`. Which model would you choose if the set of positive and negative prediction values are the concerns? If you can find an R package to do so, you may use it directly.
```{r}
library(pROC)
fit1.roc = roc(hd_data.f$HD, fit1$fitted, col="blue")
plot(fit1.roc$thresholds, 1-fit1.roc$specificities,  col="red", pch=16,  
     xlab="Threshold on prob",
     ylab="possibility",
     main = "fit1 Thresholds vs. positive/negative prediction")
points(fit1.roc$thresholds, fit1.roc$sensitivities, col="blue", pch=16, cex=.6)
legend("topright", legend=c("positive prediction", "negative prediction"),
       lty=c(1,1), lwd=c(2,2), col=c("red", "blue"))


fit2.roc = roc(hd_data.f$HD, fit2$fitted, col="blue")
plot(fit2.roc$thresholds, 1-fit2.roc$specificities,  col="red", pch=16,  
     xlab="Threshold on prob",
     ylab="possibility",
     main = "fit2 Thresholds vs. positive/negative prediction")
points(fit2.roc$thresholds, fit2.roc$sensitivities, col="blue", pch=16, cex=.6)
legend("topright", legend=c("positive prediction", "negative prediction"),
       lty=c(1,1), lwd=c(2,2), col=c("red", "blue"))


```
### Part 3 - Bayes Rule
Bayes rules with risk ratio $\frac{a_{10}}{a_{01}}=10$ or $\frac{a_{10}}{a_{01}}=1$. Use your final model obtained from 1 B) to build a class of linear classifiers.


a. Write down the linear boundary for the Bayes classifier if the risk ratio of $a_{10}/a_{01}=10$.
**Answer**: $\frac{P(Y=1|X)}{P(Y=0|X)} >\frac{a_{01}}{a_{10}}$
$logit>log(\frac{0.09}{0.90}) = -2.30$
$logit = -9.23+0.06*AGE+0.91*SEX(Male)+0.016*SBP+0.004*CHOL+0.006*FRW+0.012*CIG>-2.30$
Linear boundary:
$0.06*AGE+0.91*SEX(Male)+0.016*SBP+0.004*CHOL+0.006*FRW+0.012*CIG > 6.93$

b. What is your estimated weighted misclassification error for this given risk ratio?
```{r}
fit.final.pred.bayes <- rep("0", length(hd_data.f$HD))
fit.final.pred.bayes[fit.final$fitted > 0.09] = "1"
fit.final.pred.bayes <- as.factor(ifelse(fit.final$fitted > 0.09, "1", "0"))
MCE.bayes=(sum(10*(fit.final.pred.bayes[hd_data.f$HD == "1"] != "1")) 
           + sum(fit.final.pred.bayes[hd_data.f$HD == "0"] != "0"))/length(hd_data.f$HD)
MCE.bayes
```


c. Recall Liz, our patient from part 1. How would you classify her under this classifier?
```{r}
0.06*50+0.016*110+0.004*180+0.006*105+0.012*0 - 6.93 >0
```
**Answer**: Liz will be classified in 0, she will not have Heart Disease.
Now, draw two estimated curves where x = posterior threshold, and y = misclassification errors, corresponding to the thresholding rule given in x-axis.
```{r}
X = rep(0,100)
Y = rep(0,100)

for (i in 1:100){
  x =i/100
  fit.pred.test <- rep("0", length(hd_data.f$HD))
  fit.pred.test[fit.final$fitted >x] = "1"
  y=(sum(10*(fit.pred.test[hd_data.f$HD == "1"] != "1")) 
           + sum(fit.pred.test[hd_data.f$HD == "0"] != "0"))/length(hd_data.f$HD)
  X[i] = x
  Y[i] = y
}
plot(X,Y,col = "blue",xlab = "posterior threshold",ylab = "misclassification errors")
title("estimated curves")
```


d. Use weighted misclassification error, and set $a_{10}/a_{01}=10$. How well does the Bayes rule classifier perform? 
```{r}
fit.final.pred.bayes.10 <- rep("0", length(hd_data.f$HD))
fit.final.pred.bayes.10[fit.final$fitted > 0.09] = "1"

MCE.bayes.10=(sum(10*(fit.final.pred.bayes.10[hd_data.f$HD == "1"] != "1")) 
           + sum(fit.final.pred.bayes.10[hd_data.f$HD == "0"] != "0"))/length(hd_data.f$HD)
MCE.bayes.10

#
```

**Answer**: The MCE is small, so the Bayes rule classifier performs well.

e. Use weighted misclassification error, and set $a_{10}/a_{01}=1$. How well does the Bayes rule classifier perform? 
```{r}
fit.final.pred.bayes2 <- rep("0", length(hd_data.f$HD))
fit.final.pred.bayes2[fit.final$fitted > 0.5] = "1"
MCE_1.bayes=(sum(1*(fit.final.pred.bayes2[hd_data.f$HD == "1"] != "1")) 
           + sum(fit.final.pred.bayes2[hd_data.f$HD == "0"] != "0"))/length(hd_data.f$HD)
MCE_1.bayes
```
**Answer**: The Misclassification error is very low, so the Bayes rule classifier performs well.

## Problem 2

How well can we predict whether a bill will be passed by the legislature? 

Hundreds to thousands of bills are written each year in Pennsylvania. Some are long, others are short. Most of the bills do not even get to be voted on (“sent to the floor”). The chamber meets for 2-year sessions.  Bills that are not voted on before the end of the session (or which are voted on but lose the vote) are declared dead. Most bills die. In this study we examine about 8000 bills proposed since 2009, with the goal of building a classifier which has decent power to forecast which bills are likely to be passed. 

We have available some information about 8011 bills pertaining to legislation introduced into the Pennsylvania House of Representatives.  The goal is to predict which proposals will pass the House. Here is some information about the data:

The response is the variable called `status.` `Bill:passed` means that the bill passed the House; `governor:signed` means that the bill passed both chambers (including the House) and was enacted into law; `governor:received` means that the bill has passed both chambers and was placed before the governor for consideration.  All three of these statuses signify a success or a PASS (Meaning that the legislature passed the bill. This does not require it becoming law). All other outcomes are failures.

Here are the rest of the columns:

*	`Session` – in which legislative session was the bill introduced
*	`Sponsor_party` – the party of the legislator who sponsored the bill (every bill has a sponsor)
*	`Bill_id` – of the form HB-[bill number]-[session], e.g., `HB-2661-2013-2014` for the 2661st House Bill introduced in the 2013-2014 session.
*	`Num_cosponsors` – how many legislators cosponsored the bill
*	`Num_d_cosponsors` – how many Democrats cosponsored the bill
*	`Num_r_cosponsors` – how many Republicans cosponsored the bill
*	`Title_word_count` – how many words are in the bill’s title
*	`Originating_committee` – most bills are sent (“referred”) to a committee of jurisdiction (like the transportation committee, banking & insurance committee, agriculture & rural affairs committee) where they are discussed and amended.  The originating committee is the committee to which a bill is referred.
*	`Day_of_week_introduced` – on what day the bill was introduced in the House (1 is Monday)
*	`Num_amendments` – how many amendments the bill has
*	`Is_sponsor_in_leadership` – does the sponsor of the bill hold a position inside the House (such as speaker, majority leader, etc.)
*	`num_originating_committee_cosponsors` – how many cosponsors sit on the committee to which the bill is referred
*	`num_originating_committee_cosponsors_r` – how many Republican cosponsors sit on the committee to which the bill is referred
*	`num_originating_committee_cosponsors_d` - how many Democratic cosponsors sit on the committee to which the bill is referred

The data you can use to build the classifier is called `Bills.subset`. It contains 7011 records from the full data set. I took a random sample of 1000 bills from the 2013-2014 session as testing data set in order to test the quality of your classifier, it is called `Bills.subset.test.`

Your job is to choose a best set of classifiers such that

* The testing ROC curve pushes to the upper left corner the most, and has a competitive AUC value.
* Propose a reasonable loss function, and report the Bayes rule together with its weighted MIC. 
* You may also create some sensible variables based on the predictors or make other transformations to improve the performance of your classifier.

Here is what you need to report: 

1. Write a summary about the goal of the project. Give some background information. If desired, you may go online to find out more information.
2. Give a preliminary summary of the data. 
3. Based on the data available to you, you need to build a classifier. Provide the following information:
    *	The process of building your classifier
    *	Methods explored, and why you chose your final model
    *	Did you use a training and test set to build your classifier using the training data? If so, describe the process including information about the size of your training and test sets.
    *	What is the criterion being used to build your classifier?
    *	How do you estimate the quality of your classifier?
4. Suggestions you may have: what important features should have been collected which would have helped us to improve the quality of the classifiers.

*Final notes*: The data is graciously lent from a friend. It is only meant for you to use in this class. All other uses are prohibited without permission. 

**Answer: **See report with R code starting the next page.

\pagebreak

#Project Goal
In an era of government, Congress is where people write thousands of bills to each year but criticized to be unproductive. As mentioned in the news of $The Washington Post$ on Feb 1, 2018, members introduce about 11,000 pieces of legislation in a typical two-year term. A few hundred come to a floor vote, and only about half of those will be signed into law. Therefore, it brings attention to people what factors are involved in the success of bills. To investigate this, we develop a model to predict whether a bill will be successfully passed by the legislature using Pennsylvania as a representative. We're going to show which factors are worth paying attention to by giving them a priority for people to consider before they submit the bills.

#Preliminary Summary of Data
```{r}
# read the training and testing data
train_data <- read.csv("Bills.subset.csv")
test_data <- read.csv("Bills.subset.test.csv")
```
The whole data we use in this project containing 8011 bills which pertain to legislation introduced into the Pennsylvania House of Representatives. It is splitted into training and testing data, namely `Bills.subset` and `Bills.subset.test`, respectively. The training set contains 7011 records from the full data set, and the testing data set is a random sample of 1000 bills token from the 2013-2014 session in order to test the quality of the classifier. However, some of them has missing values. At a first glance of the dataset, we find that most of the missing values come from the `Originating_committee` predictor, which indicates the committee to which a bill is referred. To find out which columns contain missing value, we assume that missing data is coded as NA or is an empty string.
```{r results='hide'}
# find the number of missing data in training and testing data
sum(is.na(train_data))
sum(is.na(test_data))

# see which columns have missing values in training and testing set
sapply(train_data, function(x) any(is.na(x)))
sapply(train_data, function(x) any(x == ""))

sapply(train_data, function(x) any(is.na(x)))
sapply(train_data, function(x) any(x == ""))
```

We found that missing values are included in columns `day.of.week.introduced`, `status`, `sponsor_party`, and `originating_committee`. To git rid of all these columns, we first assign all empty cells in both training and testing set to NA, and then omit all the missing data by applying `na.omit`.
```{r}
train_data[train_data==""] <- NA
test_data[test_data==""] <- NA

train_data <- na.omit(train_data)
test_data <- na.omit(test_data)
```

The final training set contains `r dim(train_data)[1]` instances out of the original 7011 instances, and the final testing set contains `r dim(test_data)[1]` instead of the original 1000 instances.

The response is the variable called `status.` It includes Nine statuses which are summarized as follow. 

Bill Status | Success | Description
---------------------- | -------- | ---------------------------------------------------------------------------
`bill:passed` | Yes | Passed the House
`governor:signed` | Yes | Passed both chambers (including the House) and was enacted into law
`governor:received` | Yes | Passed both chambers and was placed before the governor for consideration
`committee:referred` | No | The bill is referred to the commitee
`committee:passed` | No | The bill passed the commitee
`amendment:passed` | No | The bill passed the amendment
`bill:reading:1` | No | The bill is being reading
`bill:reading:2` | No | The bill is being reading
`bill:reading:3` | No | The bill is being reading

We signify statuses `bill:passed`, `governor:signed`, and `governor:received` as a success or a PASS (Meaning that the legislature passed the bill. This does not require it becoming law) while all other outcomes as failures. The final training set contains 455 of successful bills and 6192 of failures, and the final testing set includes 68 successes and 931 failures.
```{r results='hide'}
# rename statuses `withbill:passed`, `governor:signed`, and `governor:received` as 1, otherwise 0
levels(train_data$status)[levels(train_data$status) == "bill:passed"] <- 1
levels(train_data$status)[levels(train_data$status) == "governor:signed"] <- 1
levels(train_data$status)[levels(train_data$status) == "governor:received"] <- 1
levels(train_data$status)[levels(train_data$status) != 1] <- 0

levels(test_data$status)[levels(test_data$status) == "bill:passed"] <- 1
levels(test_data$status)[levels(test_data$status) == "governor:signed"] <- 1
levels(test_data$status)[levels(test_data$status) == "governor:received"] <- 1
levels(test_data$status)[levels(test_data$status) != 1] <- 0

# count the number of instances of successes and failures
sum(train_data$status == 1) #455
sum(train_data$status == 0)  #6192

sum(test_data$status == 1) #68
sum(test_data$status == 0)  #931
```

There are 14 predictors that are taken into considering in the original dataset described in the table below, with some of them continuous and others categorical.

\begingroup\small

Predictors | Description
------------------------------------------------- | ---------------------------------------------------------------------------
`Session` | In which legislative session was the bill introduced
`Sponsor_party` | The party of the legislator who sponsored the bill (every bill has a sponsor)
`Bill_id` | Of the form HB-[bill number]-[session], e.g., `HB-2661-2013-2014` for the 2661st House Bill introduced in the 2013-2014 session.
`Num_cosponsors` | How many legislators cosponsored the bill
`Num_d_cosponsors` | How many Democrats cosponsored the bill
`Num_r_cosponsors` | How many Republicans cosponsored the bill
`Title_word_count` | How many words are in the bill’s title
`Originating_committee` | Most bills are sent (“referred”) to a committee of jurisdiction (like the transportation committee, banking & insurance committee, agriculture & rural affairs committee) where they are discussed and amended.  The originating committee is the committee to which a bill is referred.
`Day_of_week_introduced` | On what day the bill was introduced in the House (1 is Monday)
`Num_amendments` | How many amendments the bill has
`Is_sponsor_in_leadership` | Does the sponsor of the bill hold a position inside the House (such as speaker, majority leader, etc.)
`num_originating_committee_cosponsors` | How many cosponsors sit on the committee to which the bill is referred
`num_originating_committee_cosponsors_r` | How many Republican cosponsors sit on the committee to which the bill is referred
`num_originating_committee_cosponsors_d` | How many Democratic cosponsors sit on the committee to which the bill is referred
\endgroup

#Classification and Model Selection
First, we use logistic regression on our train dataset. By looking at the structure of the predictors, we find that `bill_id` is a factor of 7011 levels, which means that each instances has different values. We also find that the the years in `bill_id` match the predictor `session`. Therefore, we can git rid of the predictor `bill_id` since it does not give much useful information to our model.
```{r}
str(train_data)
```
We also find that `num_cosponsors` is correlated to `num_d_cosponsors` and  `num_r_cosponsors` (`num_cosponsors` = `num_d_cosponsors` + `num_r_cosponsors`), so we can get the result of one of them given the other two values. So do the predictors `num_originating_committee_cosponsors`, `num_originating_committee_cosponsors_d`, and `num_originating_committee_cosponsors_r`. Also, we find that only the ingredient of the committee matters, not the title, so we drop `originating_committee`. Thus the first model includes all predictors except `bill_id`, `num_r_cosponsors`, `num_originating_committee_cosponsors_d`, and `originating_committee`.

We use `Anova()` to drop categorical predictor which has low p-value. Then use backward selection method to keep only variables whose coefficients are significantly different from 0 at .05 level, and kick out the variable with the largest p-value first, and then re-fit the model to see if there are other variables should be kicked out.
```{r, results='hide'}
library(car)
fit.bill <- glm(status~.-bill_id -num_r_cosponsors -num_originating_committee_cosponsors_d 
                -originating_committee, train_data, family=binomial)  
summary(fit.bill)
Anova(fit.bill)

fit.bill.1 <- update(fit.bill, .~. -num_cosponsors)
summary(fit.bill.1)
fit.bill.2 <- update(fit.bill.1, .~. -num_originating_committee_cosponsors_r)
summary(fit.bill.2)
fit.bill.3 <- update(fit.bill.2, .~. -num_d_cosponsors)
summary(fit.bill.3)
fit.bill.4 <- update(fit.bill.3, .~. -day.of.week.introduced)
summary(fit.bill.4)
fit.bill.5 <- update(fit.bill.4, .~. -is_sponsor_in_leadership)
summary(fit.bill.5)
fit.bill.6 <- update(fit.bill.5, .~. -num_originating_committee_cosponsors)
summary(fit.bill.6)
```

```{r}
fit.bill.6.predict <- predict(fit.bill.6, train_data, type="response")
fit.bill.backward <- glm(status~sponsor_party+session+title_word_count+num_amendments, 
                         train_data, family = binomial)
summary(fit.bill.backward)
```

From the model we can say that, collectively, `title_word_count`, `num_amendments` are all positively related to the chance of a `status`, although the correlations are weak (the slopes are mild). Specifically, the log of status would increase by 1.7850 if `num_amendments` increased by 1. The other variables can be interperated in the same way. Also, `Republican` would have a higher chance of success than `Democratic` while all the other factors are controlled in our model, and the success also varies each session year. 

#Prediction and Analysis
We do prediction on the testing data using the model we obtain from training data. And display ROC curve and AUC (Area under the curve) to select the best classifier.

```{r}
fit.bill.predict.train <- predict(fit.bill.backward, train_data, type="response")
fit.bill.predict.test <- predict(fit.bill.backward, test_data, type="response")

library(pROC)
fit.bill.final.1 <- glm(status~sponsor_party+session+title_word_count+num_amendments, 
                        train_data, family = binomial)
fit.bill.final.2 <- glm(status~sponsor_party+title_word_count+num_amendments, train_data, 
                        family = binomial)
fit.bill.final.3 <- glm(status~sponsor_party+num_amendments, train_data, family = binomial)

bill.fit1.roc<- roc(train_data$status, fit.bill.final.1$fitted, col="green")
bill.fit2.roc<- roc(train_data$status, fit.bill.final.2$fitted, col="red")
bill.fit3.roc<- roc(train_data$status, fit.bill.final.3$fitted, col="blue")


plot(1-bill.fit1.roc$specificities, bill.fit1.roc$sensitivities, col="red", pch=16, cex=.6, 
     xlab="False Positive", 
     ylab="Sensitivity")
points(1-bill.fit2.roc$specificities, bill.fit2.roc$sensitivities, col="green", pch=16, cex=.6)
points(1-bill.fit3.roc$specificities, bill.fit3.roc$sensitivities, col="blue", pch=16, cex=.6)

title("ROC Curves for Billing Status")

pROC::auc(bill.fit1.roc)
pROC::auc(bill.fit2.roc)
pROC::auc(bill.fit3.roc)
```

We create the confusion matrix table to estimate the Positive Prediction Values and Negative Prediction Values using .5 as a threshold.
```{r}
#Positive Prediction
bill.fit.pred <- ifelse(fit.bill.final.1$fitted.values > 0.5, "1", "0")
bill.cm <- table(bill.fit.pred, train_data$status)
bill.cm
positive.pred <- bill.cm[2, 2] / (bill.cm[2, 1] + bill.cm[2, 2])
positive.pred
#Negative Prediction
negative.pred <- bill.cm[1, 1] / (bill.cm[1, 1] + bill.cm[1, 2])
negative.pred
```

We use Bayes rules with risk ratio $\frac{a_{10}}{a_{01}}=5$ or $\frac{a_{10}}{a_{01}}=1$, and use our final model to build a class of linear classifiers. The MCE is small, so the Bayes rule classifier performs well.
```{r}
bill.fit.final.pred.bayes <- rep("0", length(train_data$status))
bill.fit.final.pred.bayes[fit.bill.final.1$fitted > 0.09] = "1"
bill.fit.final.pred.bayes <- as.factor(ifelse(fit.bill.final.1$fitted > 0.09, "1", "0"))
MCE.bill.bayes=(sum(5*(bill.fit.final.pred.bayes[train_data$status == "1"] != "1")) 
           + sum(bill.fit.final.pred.bayes[train_data$status == "0"] != "0"))/length(train_data$status)
MCE.bill.bayes

bill.fit.final.pred.bayes2 <- rep("0", length(train_data$status))
bill.fit.final.pred.bayes2[fit.bill.final.1$fitted > 0.09] = "1"
bill.fit.final.pred.bayes2 <- as.factor(ifelse(fit.bill.final.1$fitted > 0.09, "1", "0"))
MCE.bill.bayes2=(sum(1*(bill.fit.final.pred.bayes2[train_data$status == "1"] != "1")) 
           + sum(bill.fit.final.pred.bayes2[train_data$status == "0"] != "0"))/length(train_data$status)
MCE.bill.bayes2
```
